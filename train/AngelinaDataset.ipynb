{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eed88c-e8aa-4060-bcbe-3852b8394d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,20))\n",
    "img = plt.imread(\"./books/chudo_derevo_redmi/IMG_20190715_112912.labeled.jpg\")\n",
    "print(img.shape)\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "#from . \n",
    "import letters\n",
    "\n",
    "reverce_dict = defaultdict(set)\n",
    "for d in letters.letter_dicts.values():\n",
    "    # print(d)\n",
    "    for lbl123, char in d.items():\n",
    "        reverce_dict[char].add(lbl123)\n",
    "        \n",
    "# print(reverce_dict)\n",
    "\n",
    "labeling_synonyms = {\n",
    "    \"xx\": \"XX\",\n",
    "    \"хх\": \"XX\",  # russian х on the left\n",
    "    \"cc\": \"CC\",\n",
    "    \"сс\": \"CC\",  # russian с on the left\n",
    "    \"<<\": \"«\",\n",
    "    \">>\": \"»\",\n",
    "    \"((\": \"()\",\n",
    "    \"))\": \"()\",\n",
    "    \"№\": \"н\",\n",
    "    \"&&\": \"§\",\n",
    "}\n",
    "\n",
    "v = [1, 2, 4, 8, 16, 32]\n",
    "\n",
    "def validate_int(int_label):\n",
    "    '''\n",
    "    Validate int_label is in [0..63]\n",
    "    Raise exception otherwise\n",
    "    '''\n",
    "    assert isinstance(int_label, int)\n",
    "    assert int_label >= 0 and int_label < 64, \"Ошибочная метка: \" + str(int_label)\n",
    "\n",
    "def label123_to_int(label123):\n",
    "    try:\n",
    "        r = sum([v[int(ch)-1] for ch in label123])\n",
    "    except:\n",
    "        raise ValueError(\"incorrect label in 123 format: \" + label123)\n",
    "    validate_int(r)\n",
    "    return r\n",
    "\n",
    "def human_label_to_int(label):\n",
    "    '''\n",
    "    Convert label from manual annotations to int_label\n",
    "    '''\n",
    "    label = label.lower()\n",
    "    if label[0] == '~':\n",
    "        label123 = label[1:]\n",
    "        if label123[-1] == '~':\n",
    "            label123 = label123[:-1]\n",
    "    else:\n",
    "        label = labeling_synonyms.get(label, label)\n",
    "        ch_list = reverce_dict.get(label, None)\n",
    "        if not ch_list:\n",
    "            raise ValueError(\"unrecognized label: \" + label)\n",
    "        if len(ch_list) > 1:\n",
    "            # raise ValueError(\"label: \" + label + \" has more then 1 meanings: \" + str(ch_list))\n",
    "            pass\n",
    "        label123 = list(ch_list)[0]\n",
    "    return label123_to_int(label123)\n",
    "\n",
    "def limiting_scaler(source, dest):\n",
    "    '''\n",
    "    Creates function to convert coordinates from source scale to dest with limiting to [0..dest)\n",
    "    :param source: source scale\n",
    "    :param dest: dest scale\n",
    "    :return: function f(x) for linear conversion [0..sousce)->[0..dest) so that\n",
    "        f(0) = 0, f(source-1) = (source-1)/source*dest, f(x<0)=0, f(x>=source) = (source-1)/source*dest\n",
    "    '''\n",
    "    def scale(x):\n",
    "        return int(min(max(0, x), source-1)) * dest/source\n",
    "    return scale\n",
    "\n",
    "def read_LabelMe_annotation(label_filename, get_points):\n",
    "    '''\n",
    "    Reads LabelMe (see https://github.com/IlyaOvodov/labelme labelling tool) annotation JSON file.\n",
    "    :param label_filename: path to LabelMe annotation JSON file\n",
    "    :return: list of rect objects. Each rect object is a tuple (left, top, right, bottom, label) where\n",
    "        left..bottom are in [0,1), label is int in [1..63]\n",
    "    '''\n",
    "    if get_points:\n",
    "        raise NotImplementedError(\"read_annotation get_point mode not implemented for LabelMe annotation\")\n",
    "    with open(label_filename, 'r', encoding='cp1251') as opened_json:\n",
    "        loaded = json.load(opened_json)\n",
    "    convert_x = limiting_scaler(loaded[\"imageWidth\"], 1.0)\n",
    "    convert_y = limiting_scaler(loaded[\"imageHeight\"], 1.0)\n",
    "    # rects = [(\n",
    "    #           # convert_x(min(xvals)),\n",
    "    #           # convert_y(min(yvals)),\n",
    "    #           # convert_x(max(xvals)),\n",
    "    #           # convert_y(max(yvals)),\n",
    "    #           # lt.human_label_to_int(label),\n",
    "    #           min(xvals),\n",
    "    #           min(yvals),\n",
    "    #           max(xvals),\n",
    "    #           max(yvals),\n",
    "    #           human_label_to_int(label),\n",
    "    #           # label,\n",
    "    #           ) for label, xvals, yvals in\n",
    "    #                 ((shape[\"label\"],\n",
    "    #                   [coords[0] for coords in shape[\"points\"]],\n",
    "    #                   [coords[1] for coords in shape[\"points\"]]\n",
    "    #                  ) for shape in loaded[\"shapes\"]\n",
    "    #                 )\n",
    "    #         ]\n",
    "    \n",
    "    boxes = [[ min(xvals), min(yvals), max(xvals), max(yvals) ]\n",
    "                    for label, xvals, yvals in\n",
    "                            ((shape[\"label\"],\n",
    "                              [coords[0] for coords in shape[\"points\"]],\n",
    "                              [coords[1] for coords in shape[\"points\"]]\n",
    "                             ) for shape in loaded[\"shapes\"]\n",
    "                            )\n",
    "            ]\n",
    "\n",
    "    labels = [human_label_to_int(label)\n",
    "                  for label, xvals, yvals in\n",
    "                            ((shape[\"label\"],\n",
    "                              [coords[0] for coords in shape[\"points\"]],\n",
    "                              [coords[1] for coords in shape[\"points\"]]\n",
    "                             ) for shape in loaded[\"shapes\"]\n",
    "                            )\n",
    "            ]\n",
    "    \n",
    "    return boxes, labels\n",
    "\n",
    "boxes, labels = read_LabelMe_annotation(\"./books/chudo_derevo_redmi/IMG_20190715_112912.labeled.json\", False) \n",
    "print(boxes, labels)\n",
    "\n",
    "# with open(\"./books/chudo_derevo_redmi/IMG_20190715_112912.labeled.json\", 'r') as f:\n",
    "#     jdata = json.load(f)\n",
    "#     pts = jdata['shapes'][0]['points']\n",
    "#     lbl = jdata['shapes'][0]['label']\n",
    "#     print(\"PTS:\", pts)\n",
    "#     print(\"LBL:\", lbl)\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# # 이미지의 크기를 잡고 이미지의 중심을 계산합니다.\n",
    "# # (h, w) = img.shape[:2]\n",
    "# # (cX, cY) = (w // 2, h // 2)\n",
    " \n",
    "# # 이미지의 중심을 중심으로 이미지를 회전합니다.\n",
    "# # M = cv2.getRotationMatrix2D((cX, cY), 0.4, 1.0)\n",
    "# # img = cv2.warpAffine(img, M, (w, h))\n",
    "\n",
    "# for (x1, y1, x2, y2, lbl) in rects:\n",
    "#     # print(x1, y1, x2, y2, lbl)\n",
    "#     img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 1)\n",
    "#     img = cv2.putText(img, str(lbl) , (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "# # # img = cv2.rectangle(img, (441, 284), (477, 343), (0,0,255), 1)\n",
    "# # # img = cv2.rectangle(img, (493, 284), (527, 343), (0,0,255), 1)\n",
    "\n",
    "# # for c in cs:\n",
    "# #     img = cv2.rectangle(img, (c.left, c.top), (c.right, c.bottom), (255,0,0), 1)\n",
    "# #     # img = cv2.rectangle(img, (500, 291), (521,337), (255, 0, 0), 1)\n",
    "\n",
    "# # img = cv2.rectangle(img, (285, 85), (313, 128), (0,0,255), 1)\n",
    "\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2a78bf-984c-463a-8cab-d4cfe06b6dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from collections import defaultdict\n",
    "# from . import letters\n",
    "\n",
    "def validate_int(int_label):\n",
    "    '''\n",
    "    Validate int_label is in [0..63]\n",
    "    Raise exception otherwise\n",
    "    '''\n",
    "    assert isinstance(int_label, int)\n",
    "    assert int_label >= 0 and int_label < 64, \"Ошибочная метка: \" + str(int_label)\n",
    "\n",
    "def label010_to_int(label010):\n",
    "    '''\n",
    "    Convert label in label010 format to int_label\n",
    "    '''\n",
    "    v = [1,2,4,8,16,32]\n",
    "    r = sum([v[i] for i in range(6) if label010[i]=='1'])\n",
    "    validate_int(r)\n",
    "    return r\n",
    "\n",
    "CellInfo = collections.namedtuple('CellInfo', \n",
    "                                  ['row', 'col',  # row and column in a symbol grid\n",
    "                                   'left', 'top', 'right', 'bottom',  # symbol corner coordinates in pixels\n",
    "                                   'label'])  # symbol label either like '246' or '010101' format\n",
    "\n",
    "def read_txt(file_txt, binary_label = True):\n",
    "    \"\"\"\n",
    "    Loads Braille annotation from DSBI annotation txt file\n",
    "    :param file_txt: filename of txt file\n",
    "    :param binary_label: return symbol label in binary format, like '010101' (if True),\n",
    "        or human readable like '246' (if False)\n",
    "    :return: tuple (\n",
    "        angle: value from 1st line of annotation file,\n",
    "        h_lines: list of horizontal lines Y-coordinates,\n",
    "        v_lines: list of vertical lines X-coordinates,,\n",
    "        cells: symbols as list of CellInfo\n",
    "    )\n",
    "    None, None, None, None for empty annotation\n",
    "    \"\"\"\n",
    "    with open(file_txt, 'r') as f:\n",
    "        l = f.readlines()\n",
    "        if len(l) < 3:\n",
    "            return None, None, None, None\n",
    "        angle = eval(l[0])\n",
    "        v_lines = list(map(eval, l[1].split(' ')))\n",
    "        assert len(v_lines)%2 == 0, (file_txt, len(v_lines))\n",
    "        h_lines = list(map(eval, l[2].split(' ')))\n",
    "        assert len(h_lines)%3 == 0, (file_txt, len(h_lines))\n",
    "        cells = []\n",
    "        for cell_ln in l[3:]:\n",
    "            cell_nums = list(cell_ln[:-1].split(' ')) # exclude last '\\n'\n",
    "            assert len(cell_nums) == 8, (file_txt, cell_ln)\n",
    "            row = eval(cell_nums[0])\n",
    "            col = eval(cell_nums[1])\n",
    "            if binary_label:\n",
    "                label = ''.join(cell_nums[2:])\n",
    "            else:\n",
    "                label = ''\n",
    "                for i, c in enumerate(cell_nums[2:]):\n",
    "                    if c == '1':\n",
    "                        label += str(i+1)\n",
    "                    else:\n",
    "                        assert c == '0', (file_txt, cell_ln, i, c)\n",
    "            left = v_lines[(col-1)*2]\n",
    "            right = v_lines[(col-1)*2+1]\n",
    "            top = h_lines[(row-1)*3]\n",
    "            bottom = h_lines[(row-1)*3+2]\n",
    "            cells.append(CellInfo(row=row, col=col,\n",
    "                                  left=left, top=top, right=right, bottom=bottom,\n",
    "                                  label=label))\n",
    "    return angle, h_lines, v_lines, cells\n",
    "\n",
    "\n",
    "def read_DSBI_annotation(label_filename, width, height, rect_margin, get_points):\n",
    "    \"\"\"\n",
    "    Loads Braille annotation from DSBI annotation txt file in albumentations format\n",
    "    :param label_filename: filename of txt file\n",
    "    :param width: image width\n",
    "    :param height: image height\n",
    "    :param rect_margin:\n",
    "    :param get_points: Points or Symbols mode\n",
    "    :return:\n",
    "        List of symbol rects if get_points==False. Each rect is a tuple (left, top, right, bottom, label) where\n",
    "        left..bottom are in [0,1], label is int in [1..63]. Symbol size is extended to rect_margin*width of symbol\n",
    "        in every side.\n",
    "        List of points rects if get_points==True. Each point is a tuple (left, top, right, bottom, label) where\n",
    "        left..bottom are in [0,1], label is 0. Width and height of point is 2*rect_margin*width of symbol\n",
    "    \"\"\"\n",
    "    _, _, _, cells = read_txt(label_filename, binary_label=True)\n",
    "    if cells is not None:\n",
    "        boxes = [\n",
    "            [(c.left - rect_margin * (c.right - c.left)) / width,\n",
    "            (c.top - rect_margin * (c.right - c.left)) / height,\n",
    "            (c.right + rect_margin * (c.right - c.left)) / width,\n",
    "            (c.bottom + rect_margin * (c.right - c.left)) / height]\n",
    "             for c in cells if c.label != '000000']\n",
    "        labels = [\n",
    "            label010_to_int(c.label)\n",
    "             for c in cells if c.label != '000000']\n",
    "            \n",
    "    else:\n",
    "        boxes, labels = [[0,0, width, height]], [0]\n",
    "\n",
    "    return boxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160092d-4b2a-4cf1-8e7f-c4a139d9ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lst = []\n",
    "val_lst = []\n",
    "\n",
    "def make_lst(mode, data_lst, data_dir):\n",
    "    if mode == 'val':\n",
    "        data_txt = data_dir + 'val.txt'\n",
    "    else:\n",
    "        data_txt = data_dir + 'train.txt'\n",
    "        \n",
    "    with open(data_txt, 'r') as f:\n",
    "        ls = f.readlines()\n",
    "        for l in ls:\n",
    "            data_lst.append(\n",
    "                {'img':data_dir+l.strip().replace('\\\\','/'), \n",
    "                 'json':data_dir+l.strip().replace('\\\\','/').replace('.jpg','.json')}\n",
    "            )\n",
    "    return data_lst\n",
    "\n",
    "def make_lst_dsbi(mode, data_lst, data_dir):\n",
    "    if mode == 'val':\n",
    "        data_txt = data_dir + 'test.txt'\n",
    "    else:\n",
    "        data_txt = data_dir + 'train.txt'\n",
    "        \n",
    "    with open(data_txt, 'r') as f:\n",
    "        ls = f.readlines()\n",
    "        for l in ls:\n",
    "            data_lst.append(\n",
    "                {'img':data_dir+'data/'+l.strip().replace('\\\\','/').replace('.jpg','+recto.jpg'), \n",
    "                 'txt':data_dir+'data/'+l.strip().replace('\\\\','/').replace('.jpg','+recto.txt')}\n",
    "            )\n",
    "    return data_lst\n",
    "\n",
    "\n",
    "train_lst = make_lst_dsbi('train', train_lst, '/opt/ml/DSBI/')\n",
    "train_lst = make_lst('train', train_lst, './books/')\n",
    "train_lst = make_lst('train', train_lst, './handwritten/')\n",
    "train_lst = make_lst('train', train_lst, './not_braille/')\n",
    "train_lst = make_lst_dsbi('val', train_lst, '/opt/ml/DSBI/')\n",
    "\n",
    "# val_lst = make_lst_dsbi('val', val_lst, '/opt/ml/DSBI/')\n",
    "val_lst = make_lst('val', val_lst, './books/')\n",
    "val_lst = make_lst('val', val_lst, './handwritten/')\n",
    "\n",
    "print(len(train_lst), len(val_lst))\n",
    "t1_img = train_lst[0]['img']\n",
    "\n",
    "if 'json' in train_lst[1].keys():\n",
    "    t1_json = train_lst[1]['json']\n",
    "    boxes, labels = read_LabelMe_annotation(t1_json, False)\n",
    "    # print(t1_img)\n",
    "    print(labels)\n",
    "    print(boxes)\n",
    "else:\n",
    "    t1_txt = train_lst[1]['txt']\n",
    "    print(t1_txt)\n",
    "    boxes, labels = read_DSBI_annotation(t1_txt, 1, 1, 0.5, False)\n",
    "    # print(t1_img)\n",
    "    print(labels)\n",
    "    print(boxes)\n",
    "\n",
    "# x = [i for i in range(65)]    \n",
    "# print(x)\n",
    "\n",
    "# pip install fiftyone\n",
    "# from tqdm import tqdm\n",
    "# import fiftyone as fo\n",
    "\n",
    "\n",
    "# dataset = fo.Dataset()\n",
    "# # dataset.default_classes = [str(i) for i in range(65)]\n",
    "# dataset.save()\n",
    "# # print(dataset)\n",
    "# samples = []\n",
    "\n",
    "# from PIL import Image\n",
    "# # for idx, trn in tqdm(enumerate(train_lst)):\n",
    "# #     # print(idx, trn)\n",
    "# #     # boxes, labels = read_LabelMe_annotation(trn['json'], False)\n",
    "\n",
    "# #     if 'json' in trn.keys():\n",
    "# #         boxes, labels = read_LabelMe_annotation(trn['json'], False)\n",
    "# #     else:\n",
    "# #         boxes, labels = read_DSBI_annotation(trn['txt'], 1, 1, 0.5, False)      \n",
    "    \n",
    "# #     # sample = fo.Sample(filepath=trn['img'])\n",
    "# #     sample = fo.Sample(filepath=trn['img'], tags=[\"train\"])\n",
    "# #     image = Image.open(sample.filepath)\n",
    "# #     image = np.array(image)\n",
    "# #     h, w, c = image.shape  \n",
    "# #     # print(c, h, w)\n",
    "    \n",
    "# #     detections = []\n",
    "    \n",
    "# #     for idx, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "# #         # print(\"box:\", (x1, y1, x2, y2), \"label:\", labels[idx])\n",
    "# #         rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
    "# #         detections.append(fo.Detection(label=str(labels[idx]), bounding_box=rel_box))    \n",
    "# #         sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "# #         # break\n",
    "\n",
    "# #     samples.append(sample)\n",
    "# #     # print(\"samples:\", samples)\n",
    "# #     # break\n",
    "\n",
    "# for idx, trn in tqdm(enumerate(val_lst)):\n",
    "#     # print(idx, trn)\n",
    "#     # boxes, labels = read_LabelMe_annotation(trn['json'], False)\n",
    "\n",
    "#     if 'json' in trn.keys():\n",
    "#         boxes, labels = read_LabelMe_annotation(trn['json'], False)\n",
    "#     else:\n",
    "#         boxes, labels = read_DSBI_annotation(trn['txt'], 1, 1, 0.5, False)      \n",
    "    \n",
    "#     sample = fo.Sample(filepath=trn['img'], tags=[\"val\"])\n",
    "#     image = Image.open(sample.filepath)\n",
    "#     image = np.array(image)\n",
    "#     h, w, c = image.shape  \n",
    "#     # print(c, h, w)\n",
    "    \n",
    "#     detections = []\n",
    "    \n",
    "#     for idx, (x1, y1, x2, y2) in enumerate(boxes):\n",
    "#         # print(\"box:\", (x1, y1, x2, y2), \"label:\", labels[idx])\n",
    "#         rel_box = [x1 / w, y1 / h, (x2 - x1) / w, (y2 - y1) / h]\n",
    "#         detections.append(fo.Detection(label=str(labels[idx]), bounding_box=rel_box))    \n",
    "#         sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "#         # break\n",
    "\n",
    "#     samples.append(sample)\n",
    "#     # print(\"samples:\", samples)\n",
    "#     # break    \n",
    "\n",
    "# print('add_samples...')\n",
    "# dataset.add_samples(samples)\n",
    "# dataset.save()  # must save after edits\n",
    "\n",
    "# # splits = [\"train\", \"val\"]\n",
    "# splits = [\"val\"]\n",
    "\n",
    "# print('export...')\n",
    "\n",
    "# # classes = [\"list\", \"of\", \"classes\"] #[str(i) for i in range(65)]\n",
    "\n",
    "# for split in splits:\n",
    "#     split_view = dataset.match_tags(split)\n",
    "#     # print(dir(split_view))\n",
    "#     print(split_view.to_json())\n",
    "    \n",
    "#     split_view.export(       \n",
    "#         export_dir = \"../DATA_YOLO3/\",\n",
    "#         # dataset_type = fo.types.YOLOv4Dataset,\n",
    "#         dataset_type = fo.types.YOLOv5Dataset,\n",
    "#         label_field = \"ground_truth\",\n",
    "#         # labels_path = \"labels\"\n",
    "#         # classes=classes,\n",
    "#     )\n",
    "\n",
    "# # Export the dataset\n",
    "# # dataset.export(\n",
    "# #     export_dir=\"../DATA_COCO/\",\n",
    "# #     dataset_type=fo.types.COCODetectionDataset,\n",
    "# #     label_field=\"ground_truth\",\n",
    "# # )\n",
    "\n",
    "# print('finished')\n",
    "\n",
    "# print(fo.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76949e19-25cc-4a87-98e7-a6827da9a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/IlyaOvodov/AngelinaReader/blob/master/data_utils/data.py\n",
    "#box 모델이 필요할듯.. nosegmentation\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# from albumentations import bbox_utils\n",
    "\n",
    "print(A.__version__)\n",
    "\n",
    "# x = bbox_utils.convert_bboxes_to_albumentations\n",
    "# def x(bboxes, source_format, rows, cols, check_validity=False):\n",
    "#     bbox_utils.convert_bboxes_to_albumentations(bboxes, source_format, rows, cols, False)\n",
    "    \n",
    "# def check_bbox(bbox, maxsize=510.):\n",
    "#     \"\"\"Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums\"\"\"\n",
    "#    #my added block \n",
    "#     bbox=list(bbox)\n",
    "#     for i in range(4):\n",
    "#         print(bbox[i])\n",
    "#         if (bbox[i]<0.) :\n",
    "#             bbox[i]=0.\n",
    "#         elif (bbox[i]>maxsize) :\n",
    "#             bbox[i]=maxsize\n",
    "#     return tuple(bbox)\n",
    "\n",
    "class BrailleDataset(Dataset):\n",
    "    def __init__(self, file_list, mode, transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # print(\"index\", index, self.file_list[index]['json'])\n",
    "\n",
    "        image = cv2.imread(self.file_list[index]['img'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        \n",
    "        # image = cv2.resize(image, (1024, 1024))\n",
    "\n",
    "        # boxes, labels = read_LabelMe_annotation(self.file_list[index]['json'], False)\n",
    "        \n",
    "        if 'json' in self.file_list[index].keys():\n",
    "            boxes, labels = read_LabelMe_annotation(self.file_list[index]['json'], False)\n",
    "        else:\n",
    "            boxes, labels = read_DSBI_annotation(self.file_list[index]['txt'], 1, 1, 0.5, False)\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            # print(len(boxes), len(labels))\n",
    "            comboxes = []\n",
    "            for idx, box in enumerate(boxes):\n",
    "                # print(idx, box, labels[idx])\n",
    "                box.append(labels[idx])\n",
    "                comboxes.append(box)\n",
    "\n",
    "            # print(comboxes)\n",
    "            transform = A.Compose([\n",
    "                # A.RandomCrop(width=1024, height=1024, p=0.7),\n",
    "                # A.RandomCrop(width=512, height=512, p=0.5),\n",
    "                # A.RandomCrop(width=256, height=256, p=0.3),\n",
    "                A.RandomCrop(width=512, height=512, p=1.0),\n",
    "                A.RandomCrop(width=256, height=256, p=0.5),\n",
    "\n",
    "                # A.RandomCrop(width=256, height=256, p=0.5),\n",
    "                # A.RandomSizedBBoxSafeCrop(width=512, height=512),\n",
    "                # A.HorizontalFlip(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.Rotate(limit=(-8,8), p=0.5),\n",
    "            # ], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "            ], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=1)) #, check_each_transform=False))\n",
    "            # ], bbox_params=A.BboxParams(format='pascal_voc', min_area=1024, min_visibility=1, label_fields=['class_labels']))\n",
    "\n",
    "            img_w = image.shape[0]\n",
    "            img_h = image.shape[1]\n",
    "\n",
    "            if img_w >= 512 and img_h >= 512:\n",
    "            # if img_w > 1024 and img_h > 1024:\n",
    "                transformed = transform(image=image, bboxes=comboxes)\n",
    "                image = transformed['image']\n",
    "\n",
    "                # print(transformed['bboxes'])\n",
    "                boxes, labels = [], []\n",
    "                for target in transformed['bboxes']: \n",
    "                    # ck_box = check_bbox(target[:4], 510.)\n",
    "                    # boxes.append(ck_box)\n",
    "                    boxes.append(target[:4])\n",
    "                    labels.append(target[4])\n",
    "            \n",
    "        image = torch.tensor(image).permute(2,0,1)\n",
    "        \n",
    "        # print(len(labels))\n",
    "        \n",
    "        # if len(labels) == 0:\n",
    "        #     return image, {\"boxes\":torch.tensor([[0,0,0,0]]), \"labels\":torch.tensor([0])}\n",
    "        #     # return image, {\"boxes\":torch.tensor([[0,0,0,0]]), \"labels\":torch.tensor([0])}\n",
    "        # else:\n",
    "        return image, {\"boxes\":torch.tensor(boxes).float(), \"labels\":torch.tensor(labels)}\n",
    "\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "#         image_id = self.coco.getImgIds(imgIds=index)\n",
    "#         image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "#         # cv2 를 활용하여 image 불러오기\n",
    "#         images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "#         images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "#         images /= 255.0\n",
    "        \n",
    "#         if (self.mode in ('train', 'val')):\n",
    "#             ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "#             anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "#             # Load the categories in a variable\n",
    "#             cat_ids = self.coco.getCatIds()\n",
    "#             cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "#             # masks : size가 (height x width)인 2D\n",
    "#             # 각각의 pixel 값에는 \"category id\" 할당\n",
    "#             # Background = 0\n",
    "#             masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "#             # General trash = 1, ... , Cigarette = 10\n",
    "#             anns = sorted(anns, key=lambda idx : idx['area'], reverse=True)\n",
    "#             for i in range(len(anns)):\n",
    "#                 className = get_classname(anns[i]['category_id'], cats)\n",
    "#                 pixel_value = category_names.index(className)\n",
    "#                 masks[self.coco.annToMask(anns[i]) == 1] = pixel_value\n",
    "#             masks = masks.astype(np.int8)\n",
    "                        \n",
    "#             # transform -> albumentations 라이브러리 활용\n",
    "#             if self.transform is not None:\n",
    "#                 transformed = self.transform(image=images, mask=masks)\n",
    "#                 images = transformed[\"image\"]\n",
    "#                 masks = transformed[\"mask\"]\n",
    "#             return images, masks, image_infos\n",
    "        \n",
    "#         if self.mode == 'test':\n",
    "#             # transform -> albumentations 라이브러리 활용\n",
    "#             if self.transform is not None:\n",
    "#                 transformed = self.transform(image=images)\n",
    "#                 images = transformed[\"image\"]\n",
    "#             return images, image_infos\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.file_list)\n",
    "        # 전체 dataset의 size를 return\n",
    "        # return len(self.coco.getImgIds())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec492b13-fe8a-497c-87ba-f3e35a12ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = BrailleDataset(file_list=train_lst, mode='train', transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn) #False)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(len(train_loader))\n",
    "image, targets = next(iter(train_loader))\n",
    "# print(targets[0]['boxes'], targets[0]['labels'])\n",
    "# print(image[0].shape)\n",
    "\n",
    "# plt.imshow(image[0].permute(1,2,0).cpu())\n",
    "# plt.show()\n",
    "\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "# image /= 255.0\n",
    "\n",
    "img = image[0].permute(1,2,0).cpu()\n",
    "img = np.array(img)\n",
    "# print(img.shape)\n",
    "\n",
    "for idx, b in enumerate(targets[0]['boxes']):\n",
    "    box = b.cpu().detach().numpy()\n",
    "    x1 = box[0]\n",
    "    y1 = box[1]\n",
    "    x2 = box[2]\n",
    "    y2 = box[3]\n",
    "    lbl = targets[0]['labels'][idx].cpu().detach().numpy()\n",
    "    # print(x1, y1, x2, y2, lbl, scr)\n",
    "    img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 1)\n",
    "    img = cv2.putText(img, str(lbl) , (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ade3c7-5fdd-4bb4-879f-05940e00e9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR YOLO DATASETS\n",
    "\n",
    "def make_yolo_dataset(prefix4name, base_img_dir, base_txt_dir, loader, filters):\n",
    "    cnt = 0\n",
    "    for no, (images, targets) in enumerate(loader):\n",
    "        img = images[0].permute(1, 2, 0)\n",
    "        img = img.detach().cpu().numpy()\n",
    "        # width, height, ch = img.shape\n",
    "        height, width, ch = img.shape\n",
    "        # print(width, height, ch)\n",
    "        # plt.imshow(img)\n",
    "\n",
    "        lst = []\n",
    "        lbl_lst = []\n",
    "        for idx, b in enumerate(targets[0]['boxes']):\n",
    "            box = b.cpu().detach().numpy()\n",
    "            x1 = box[0]\n",
    "            y1 = box[1]\n",
    "            x2 = box[2]\n",
    "            y2 = box[3]\n",
    "\n",
    "            w = x2-x1\n",
    "            h = y2-y1\n",
    "            cx = x1 + w/2\n",
    "            cy = y1 + h/2\n",
    "\n",
    "            lbl = targets[0]['labels'][idx].cpu().detach().numpy()\n",
    "            lst.append(f\"{lbl} {cx/width} {cy/height} {w/width} {h/height}\")\n",
    "            lbl_lst.append(int(lbl))            \n",
    "\n",
    "        #filter를 사용할 경우\n",
    "        if filters!=None:\n",
    "            for l in lbl_lst:\n",
    "                 if l in filters:\n",
    "                    with open(f\"{base_txt_dir}/{prefix4name}img{no}.txt\", \"w\") as file:\n",
    "                        for l in lst:\n",
    "                            file.writelines(l+'\\n')\n",
    "                            \n",
    "                    plt.imsave(f\"{base_img_dir}/{prefix4name}img{no}.jpeg\", img)\n",
    "                    cnt += 1\n",
    "                    break\n",
    "                    \n",
    "        #filter를 사용하지 않는 경우\n",
    "        else:\n",
    "            with open(f\"{base_txt_dir}/{prefix4name}img{no}.txt\", \"w\") as file:\n",
    "                for l in lst:\n",
    "                    file.writelines(l+'\\n')\n",
    "\n",
    "            plt.imsave(f\"{base_img_dir}/{prefix4name}img{no}.jpeg\", img)\n",
    "            cnt += 1\n",
    "\n",
    "    print(f\"{cnt} files saved \", base_img_dir, base_txt_dir)\n",
    "\n",
    "train_dataset = BrailleDataset(file_list=train_lst, mode='train', transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0, collate_fn=collate_fn) #False)\n",
    "\n",
    "base_img_dir = '/opt/ml/DATA_YOLON2/images/train'\n",
    "base_txt_dir = '/opt/ml/DATA_YOLON2/labels/train'\n",
    "\n",
    "# filters = [24]\n",
    "filters = [16, 48, 56]\n",
    "# make_yolo_dataset('test_org_', base_img_dir, base_txt_dir, train_loader, filters=None)\n",
    "# make_yolo_dataset('test_fil1_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('1N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('2N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('3N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('4N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('5N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('6N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('7N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('8N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('9N', base_img_dir, base_txt_dir, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd252f73-d3a7-4437-9a4d-47631089e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_yolo_dataset('test_fil2_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil3_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil4_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil5_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil6_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil7_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil8_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil9_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil10_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil11_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil12_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil13_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil20_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil21_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil22_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil23_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil30_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil31_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil32_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil33_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil40_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil41_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil42_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil43_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil50_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil51_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil52_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil53_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil60_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil61_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil62_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil63_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil70_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil71_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil72_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil73_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil80_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil81_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil82_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil83_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('test_fil90_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil91_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil92_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "# make_yolo_dataset('test_fil93_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "make_yolo_dataset('test_filcof11_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "make_yolo_dataset('test_filcof12_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "make_yolo_dataset('test_filcof13_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "make_yolo_dataset('test_filcof14_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "make_yolo_dataset('test_filcof15_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "make_yolo_dataset('test_filcof16_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "make_yolo_dataset('test_filcof17_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "make_yolo_dataset('test_filcof18_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "make_yolo_dataset('test_filcof19_', base_img_dir, base_txt_dir, train_loader, filters=filters)\n",
    "\n",
    "# make_yolo_dataset('10', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('11N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('12N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('13N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('14N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('15N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('16N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('17N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('18N', base_img_dir, base_txt_dir, train_loader)\n",
    "# make_yolo_dataset('19N', base_img_dir, base_txt_dir, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc0030-8c29-4942-add8-fc6df40027ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check YOLO Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "path = '/opt/ml/DATA_YOLON3/labels/train'\n",
    "path_img = '/opt/ml/DATA_YOLON3/images/train'\n",
    "# path = '/opt/ml/DATA_NEW/train/labels'\n",
    "# path_img = '/opt/ml/DATA_NEW/valid/images'\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "# print(filenames)\n",
    "lst = []\n",
    "for filename in filenames:\n",
    "    # fn, ext = filename.split('.')\n",
    "    # # print(fn, ext)\n",
    "    # if ext == 'txt':\n",
    "    # print(filename)\n",
    "    if '.ipynb_checkpoints' != filename:\n",
    "        with open(path + '/' + filename, 'r') as f:\n",
    "            ls = f.readlines()\n",
    "            for arr in ls:\n",
    "                for e in arr.split(' '):\n",
    "                    lst.append(int(e))\n",
    "                    break\n",
    "                    \n",
    "x = pd.Series(lst)\n",
    "ls = x.value_counts()\n",
    "# print(ls)\n",
    "\n",
    "dict = {k:0 for k in range(65)}\n",
    "# print(dict)\n",
    "\n",
    "for l in lst:\n",
    "    dict[l] += 1\n",
    "    \n",
    "# print(dict)\n",
    "plt.plot(dict.values())\n",
    "\n",
    "keys = []\n",
    "for key, val in dict.items():\n",
    "    if val < 1000:\n",
    "        print(key, val)\n",
    "        if key not in [0, 64]:\n",
    "            keys.append(key)\n",
    "\n",
    "print(keys)        \n",
    "# x.hist()\n",
    "# import matplotlib.pyplot as plt\n",
    "# # plt.figure(figsize=(15,10))\n",
    "# plt.hist(lst, bins=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f16a0-bdeb-4bfd-af59-27d4eec56322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(val_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222e840-a72e-4dfd-8c7c-84f402ab2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = BrailleDataset(file_list=train_lst, mode='train', transform=None)\n",
    "val_dataset = BrailleDataset(file_list=val_lst, mode='val', transform=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn) #False)\n",
    "\n",
    "base_img_dir = '/opt/ml/DATA_YOLON2/images/val2'\n",
    "base_txt_dir = '/opt/ml/DATA_YOLON2/labels/val2'\n",
    "\n",
    "# make_yolo_dataset('', base_img_dir, base_txt_dir, val_loader)\n",
    "make_yolo_dataset('test_', base_img_dir, base_txt_dir, val_loader, filters=None)\n",
    "# filters=[4, 12, 20, 28, 32, 35, 39, 41, 44, 57, 59, 61]\n",
    "# make_yolo_dataset('test_', base_img_dir, base_txt_dir, val_loader, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318d9f1-d943-4892-b72c-322b64639cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "num_classes = 65 # class 개수= 10 + background\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "print(torchvision.__version__)\n",
    "\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(num_classes = 65, pretrained=False, pretrained_backbone = True)\n",
    "# model = torchvision.models.detection.retinanet_resnet50_fpn_v2(num_classes = 65, pretrained=False, pretrained_backbone = True)\n",
    "\n",
    "#version update 0.8.1 -> 0.14\n",
    "#pip install --upgrade torchvision==0.14\n",
    "#retinanet_resnet50_fpn_v2\n",
    "#fcos_resnet50_fpn \n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4561acf-4637-41ef-8951-64f4991870d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "#https://github.com/IlyaOvodov/pytorch-retinanet/blob/af75970bac9faec8fedb37a21868bd308b5e9488/train.py\n",
    "# import loss\n",
    "from loss import FocalLoss\n",
    "criterion = FocalLoss(num_classes=65)\n",
    "\n",
    "t = torch.tensor([1, 2, 3])\n",
    "tt = []\n",
    "tt.append(t)\n",
    "# tt.append(t)\n",
    "\n",
    "l = torch.tensor([1, 2, 3])\n",
    "ll = []\n",
    "ll.append(l)\n",
    "# ll.append(l)\n",
    "\n",
    "loc_preds = torch.stack(tt)\n",
    "loc_targets = torch.stack(tt)\n",
    "cls_preds = torch.stack(ll)\n",
    "cls_targets = torch.stack(ll)\n",
    "\n",
    "print(loc_targets.shape, loc_preds.shape)\n",
    "print(cls_targets.shape, cls_preds.shape)\n",
    "\n",
    "# loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets)\n",
    "# print(loss)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408dae6-dfb5-4433-bb67-98d9459e1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "def train_fn(num_epochs, train_data_loader, val_data_loader, optimizer, model, device, lr, batch_size):\n",
    "\n",
    "    wandb.init(project=\"braille\", entity=\"zergswim_proj\")    \n",
    "    wandb.config = {\n",
    "      \"learning_rate\": lr,\n",
    "      \"epochs\": num_epochs,\n",
    "      \"batch_size\": batch_size\n",
    "    }    \n",
    "    \n",
    "    best_loss = 1000\n",
    "    loss_hist = Averager()\n",
    "    val_hist = Averager()\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_hist.reset()\n",
    "\n",
    "        model.train()\n",
    "        # for images, targets, image_ids in tqdm(train_data_loader):\n",
    "        for images, targets in tqdm(train_data_loader):\n",
    "\n",
    "            # gpu 계산을 위해 image.to(device)\n",
    "            images = list(image.float().to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            # targets = list(targets)\n",
    "            # print(targets)\n",
    "\n",
    "            # calculate loss\n",
    "            # print(images[0].shape, targets[0]['boxes'], targets[0]['labels'])\n",
    "            # print(targets)\n",
    "            for idx, t in enumerate(targets):\n",
    "                if len(t['boxes']) == 0:\n",
    "                    targets[idx]['boxes'] = torch.tensor([[0.,0.,1.,1.]]).float().to(device)\n",
    "                    targets[idx]['labels'] = torch.tensor([0]).to(device)\n",
    "            \n",
    "            # print(targets)\n",
    "            loss_dict = model(images, targets)\n",
    "            # torchvision.ops.sigmoid_focal_loss\n",
    "\n",
    "            # print(type(loss_dict), len(loss_dict))\n",
    "            # print(loss_dict)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_value = losses.item()\n",
    "\n",
    "            loss_hist.send(loss_value)\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Train Epoch #{epoch+1} loss: {loss_hist.value}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_hist.reset()            \n",
    "            # model.eval()\n",
    "            for images, targets in tqdm(val_data_loader):\n",
    "\n",
    "                images = list(image.float().to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                loss_dict = model(images, targets)\n",
    "\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                loss_value = losses.item()\n",
    "\n",
    "                val_hist.send(loss_value)\n",
    "\n",
    "                # backward\n",
    "                # optimizer.zero_grad()\n",
    "                # losses.backward()\n",
    "                # optimizer.step()\n",
    "\n",
    "            print(f\"Val. Epoch #{epoch+1} loss: {val_hist.value}\")                \n",
    "\n",
    "            wandb.log({\"Epoch\": epoch, \n",
    "                       \"Loss\": loss_hist.value,\n",
    "                       \"Val. Loss\": val_hist.value,\n",
    "                      })            \n",
    "            \n",
    "#                 loc_preds, loc_targets, cls_preds, cls_targets = [], [], [], []\n",
    "                \n",
    "#                 # print(targets[0]['boxes'])\n",
    "                \n",
    "#                 for idx, tar in enumerate(targets):\n",
    "#                     tar_box = tar['boxes']\n",
    "#                     tar_lbl = tar['labels'] #torch.nn.functional.one_hot(tar['labels'], num_classes=65)\n",
    "#                     loc_targets.append(tar_box)\n",
    "#                     cls_targets.append(tar_lbl)\n",
    "\n",
    "#                     if len(outputs)-1 > idx:\n",
    "#                         out_box = outputs[idx]['boxes']\n",
    "#                         out_lbl = outputs[idx]['labels'] #torch.nn.functional.one_hot(outputs[idx]['labels'], num_classes=65)\n",
    "#                         loc_preds.append(out_box)\n",
    "#                         cls_preds.append(out_lbl)\n",
    "#                     else:\n",
    "#                         zero_boxes = torch.zeros_like(tar_box)\n",
    "#                         zero_labels = torch.zeros_like(tar_lbl)\n",
    "#                         loc_preds.append(zero_boxes)\n",
    "#                         cls_preds.append(zero_labels)\n",
    "\n",
    "#                 print(len(loc_targets), len(loc_preds), len(cls_targets), len(cls_preds))\n",
    "                \n",
    "#                 for l in loc_preds:\n",
    "#                     print(l.shape)\n",
    "                        \n",
    "#                 loc_preds = torch.stack(loc_preds)\n",
    "#                 loc_targets = torch.stack(loc_targets)\n",
    "#                 cls_preds = torch.stack(cls_preds)\n",
    "#                 cls_targets = torch.stack(cls_targets)\n",
    "                \n",
    "#                 print(loc_targets.shape, loc_preds.shape)\n",
    "#                 print(cls_targets.shape, cls_preds.shape)\n",
    "\n",
    "#                 loss = criterion(loc_preds, loc_targets, cls_preds, cls_targets)\n",
    "#                 print(loss)\n",
    "                \n",
    "#                 # loss = torchvision.ops.sigmoid_focal_loss(output, targets)\n",
    "#                 # print(loss)\n",
    "#                 val_hist.update(loss.detach().item(), batch_size)\n",
    "        \n",
    "#             print(f\"Val. Epoch #{epoch+1} loss: {val_hist.value}\")\n",
    "\n",
    "#         if val_hist.value < best_loss:\n",
    "#             save_path = f'./save/retina_v2_e{epoch+1}.pth'\n",
    "#             save_dir = os.path.dirname(save_path)\n",
    "#             if not os.path.exists(save_dir):\n",
    "#                 os.makedirs(save_dir)\n",
    "            \n",
    "#             torch.save(model.state_dict(), save_path)\n",
    "#             best_loss = val_hist.value\n",
    "#             print('file saved', val_hist.value, '(best:', best_loss,')')  \n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 16\n",
    "lr = 0.00001\n",
    "train_dataset = BrailleDataset(file_list=train_lst, mode='train', transform=None)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn) #False)\n",
    "\n",
    "val_dataset = BrailleDataset(file_list=val_lst, mode='val', transform=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=collate_fn) #False)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "optimizer = torch.optim.AdamW(params, lr=lr)\n",
    "\n",
    "checkpoint = torch.load('./save/retina_v1_base.pth', map_location=device)\n",
    "# state_dict = checkpoint.state_dict()\n",
    "model.load_state_dict(checkpoint)\n",
    "print(\"model loaded\")\n",
    "\n",
    "train_fn(num_epochs, train_loader, val_loader, optimizer, model, device, lr, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59888297-962b-48e2-b6d3-446fde17d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f'./save/retina_v1_base.pth'\n",
    "# torch.save(model.state_dict(), save_path)\n",
    "# print('file saved')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f1fd6-cb73-4a76-9f39-896788819f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# # For training\n",
    "# images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n",
    "# boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n",
    "# labels = torch.randint(1, 91, (4, 11))\n",
    "# images = list(image for image in images)\n",
    "# targets = []\n",
    "# for i in range(len(images)):\n",
    "#     d = {}\n",
    "#     d['boxes'] = boxes[i]\n",
    "#     d['labels'] = labels[i]\n",
    "#     targets.append(d)\n",
    "    \n",
    "# # print(type(targets), targets[0]['boxes'], targets[0]['labels'])#, targets['boxes'])\n",
    "# # print(images[0].shape, targets)\n",
    "# output = model(images, targets)\n",
    "# print(output)\n",
    "# # >>> # For inference\n",
    "# # # >>> model.eval()\n",
    "# # # >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "# # # >>> predictions = model(x)\n",
    "# # # >>>\n",
    "# # # >>> # optionally, if you want to export the model to ONNX:\n",
    "# # # >>> torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8fa67f-0899-4bb7-b956-ad5d300348b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_result(image, limit = 0.5):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image = torch.tensor(image).permute(2,0,1)\n",
    "    image = image.unsqueeze(dim=0)\n",
    "    # image = image.reshape(1,3,1376,1024)\n",
    "    image = image.float().to(device)\n",
    "    # print(image.shape)\n",
    "\n",
    "    model.eval()\n",
    "    output = model(image)\n",
    "    # print(output)\n",
    "\n",
    "    img = image.squeeze().permute(1,2,0).cpu()\n",
    "    img = np.array(img)\n",
    "    # print(img.shape)\n",
    "\n",
    "    for idx, b in enumerate(output[0]['boxes']):\n",
    "        box = b.cpu().detach().numpy()\n",
    "        x1 = box[0]\n",
    "        y1 = box[1]\n",
    "        x2 = box[2]\n",
    "        y2 = box[3]\n",
    "        lbl = output[0]['labels'][idx].cpu().detach().numpy()\n",
    "        scr = output[0]['scores'][idx].cpu().detach().numpy()\n",
    "        # print(x1, y1, x2, y2, lbl, scr)\n",
    "        if scr > limit:\n",
    "            img = cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0,0,255), 1)\n",
    "            img = cv2.putText(img, str(lbl) , (int(x1), int(y1)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n",
    "\n",
    "    plt.figure(figsize=(10,20))\n",
    "    plt.imshow(img)\n",
    "    \n",
    "image = plt.imread(\"./books/chudo_derevo_redmi/IMG_20190715_112936.labeled.jpg\")\n",
    "# image = plt.imread(\"./books/chudo_derevo_redmi/IMG_20190715_112912.labeled.jpg\")\n",
    "print(type(image))\n",
    "show_result(image, limit=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3719ef-0be5-46c2-894c-a0d04d260dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = plt.imread(\"/opt/ml/DSBI/data/The Second Volume of Ninth Grade Chinese Book 1/SVNGCB1+9.jpg\")\n",
    "image = plt.imread(\"/opt/ml/DSBI/data/Fundamentals of Massage/FM+1.jpg\")\n",
    "# image = plt.imread(\"Korean_Braille.jpg\")\n",
    "# image = plt.imread(\"test.jpg\")\n",
    "# image = cv2.resize(image, (512, 512))\n",
    "\n",
    "show_result(image, limit=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131dec45-567d-4557-98ad-727ae62a55dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = plt.imread(\"Korean_Braille.jpg\")\n",
    "# image = plt.imread(\"test.jpg\")\n",
    "# image = cv2.resize(image, (512, 512))\n",
    "\n",
    "show_result(image, limit=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998bf8b-e9bb-4d61-b03a-6290f536702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'https://test.narangdesign.com/mail/kbuwel/202011/images/news_01_img1.jpg'\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "response = requests.get(img_path)\n",
    "raw_img = Image.open(BytesIO(response.content))\n",
    "# plt.imshow(raw_img)\n",
    "\n",
    "image = np.array(raw_img)\n",
    "image = cv2.resize(image, (1024, 1024))\n",
    "\n",
    "show_result(image, limit=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc933440-fb9b-4a4f-ad36-79208e0a1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = BrailleDataset(file_list=val_lst, mode='val', transform=None)\n",
    "# val_data_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1, collate_fn=collate_fn) #False)\n",
    "\n",
    "# model.eval()\n",
    "# # val_hist = Averager()\n",
    "# with torch.no_grad():\n",
    "#     # val_hist.reset()            \n",
    "#     for images, targets in tqdm(val_data_loader):\n",
    "\n",
    "#         images = list(image.float().to(device) for image in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#         loss_dict = model(images, targets)\n",
    "        \n",
    "#         print(loss_dict)\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "# #         losses = sum(loss for loss in loss_dict.values())\n",
    "# #         loss_value = losses.item()\n",
    "\n",
    "# #         val_hist.send(loss_value)\n",
    "\n",
    "#         # backward\n",
    "#         # optimizer.zero_grad()\n",
    "#         # losses.backward()\n",
    "#         # optimizer.step()\n",
    "\n",
    "#     # print(f\"Val. Epoch #{epoch+1} loss: {val_hist.value}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8739f8-eda2-4d03-bcb4-53302c2bf043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ucd = '\\u2801'\n",
    "# ucd2 = '\\u2803'\n",
    "# print(ucd, ord(ucd))\n",
    "# print(ucd2, ord(ucd2))\n",
    "\n",
    "bind = []\n",
    "for i in range(1, 64):\n",
    "    ucd = chr(10240+i)\n",
    "    str = '{0:2}'.format(i)\n",
    "    bind.append((str, ucd))\n",
    "    if i%10==0:\n",
    "        print(bind)\n",
    "        bind = []\n",
    "print(bind)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d5c87-aa19-4a4b-b167-228d0d6d3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = bin(2800+1).encode('utf-8')\n",
    "print(c)\n",
    "print(c.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a153d4-48f4-4ad6-bdb5-292f12318201",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"2진수: {0:#b}, 8진수: {0:#o}, 10진수: {0:#d}, 16진수: {0:#x}\".format(60)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434da7c-c745-427d-a0e1-391bce1fcfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unichr('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfbb3b-0809-4e48-b8b9-2a34b0ac0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Create black mask using Numpy and convert from BGR (OpenCV) to RGB (PIL)\n",
    "# image = cv2.imread('1.png') # If you were using an actual image\n",
    "image = np.zeros((500, 500, 3), dtype=np.uint8)\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "pil_image = Image.fromarray(image)\n",
    "\n",
    "# Draw non-ascii text onto image\n",
    "font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSerif-Bold.ttf\", 20)\n",
    "draw = ImageDraw.Draw(pil_image)\n",
    "draw.text((100, 10), chr(10240+63), font=font)\n",
    "plt.imshow(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03625f-767c-4ccb-9728-1b486033a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fc-list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68856056-5390-4757-9f91-eb543b0beb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6baac8-9145-4f29-902f-381c7f9ba5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
